{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"phm\"\n",
    "method_name = \"phm\"\n",
    "pp_threshold = 0.8\n",
    "chromosomes = [\"chr22\"]\n",
    "dataset = \"test_data\"\n",
    "save_files = True\n",
    "\n",
    "# to use TAD and AB compartment information, set use_tad_ab_info = True\n",
    "# Make sure to prepare the data in the correct format!\n",
    "# (see Jupyter notebooks in Chromatin_modules/genome_annotations/TADs_AB_compartments)\n",
    "use_tad_ab_info = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_path = \"/data/pushkare/Chromatin_modules\"\n",
    "# Input data paths\n",
    "path_to_cm_peaks = os.path.join(core_path, \"2.peaks_in_CMs\")\n",
    "overlapping_peaks_path = os.path.join(\n",
    "    core_path,\n",
    "    \"2.peaks_in_CMs\",\n",
    "    \"peak_overlaps\",\n",
    "    \"_\".join([\"overlap_of\", method, \"and_not\", method, \"peaks\"]),\n",
    ")\n",
    "\n",
    "count_mtx_path = os.path.join(core_path, \"test_data\")\n",
    "if use_tad_ab_info:\n",
    "    tads_ab_path = os.path.join(core_path, \"genome_annotations\", \"TADs_AB_compartments\")\n",
    "# Path to store input data for simulation\n",
    "output_data_path = os.path.join(\n",
    "    core_path,\n",
    "    \"3.simulate_CMs\",\n",
    "    \"input\",\n",
    "    method,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_peak_df = pd.read_csv(\n",
    "    os.path.join(output_data_path, dataset + \"_single_peak_ids.txt\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"chromosome\", \"peak_start\", \"peak_end\", \"mark\", \"peak_id\"],\n",
    ")\n",
    "\n",
    "# Prepare data for CM peaks\n",
    "cm_peaks_df = utils.read_bed(\n",
    "    path_to_file_dir=os.path.join(path_to_cm_peaks, \"peak_files\"),\n",
    "    file_name=\"_\".join([dataset, method, \"all_peaks.bed\"]),\n",
    "    sep=\"\\t\",\n",
    "    add_chr=True,\n",
    "    add_chr_col=\"chr\",\n",
    "    columns=[\"chr\", \"peak_start\", \"peak_end\", \"peak_id\", \"cm_id\", \"strand\"],\n",
    "    header=None,\n",
    ")\n",
    "cm_peaks_df.loc[:, \"cm_id\"] = cm_peaks_df.loc[:, \"cm_id\"].str.replace(\"_\", \"~\")\n",
    "n_peaks_per_cm_dict = cm_peaks_df.groupby(\"cm_id\").size().to_dict()\n",
    "cm_peak_coordinates_by_chr_dict = utils.get_peak_coord_in_cms_by_chr(\n",
    "    cm_peaks_df=cm_peaks_df,\n",
    "    chr_col=\"chr\",\n",
    "    peak_start_col=\"peak_start\",\n",
    "    peak_end_col=\"peak_end\",\n",
    "    cm_id_col=\"cm_id\",\n",
    ")\n",
    "cm_peaks_dict = utils.get_peak_ids_by_cm(\n",
    "    peak_df=cm_peaks_df, peak_id_col=\"peak_id\", cm_id_col=\"cm_id\"\n",
    ")\n",
    "mark_counts_dict = {\n",
    "    ref_cm_id.replace(\"_\", \"~\"): {\n",
    "        mark: n_peaks / n_peaks_per_cm_dict[ref_cm_id]\n",
    "        for mark, n_peaks in collections.Counter(\n",
    "            [ref_peak.split(\":\")[0] for ref_peak in cm_peaks]\n",
    "        ).items()\n",
    "    }.get(\"H3K27ac\", 0.0)\n",
    "    for ref_cm_id, cm_peaks in cm_peaks_dict.items()\n",
    "}\n",
    "\n",
    "# Prepare data for not CM peaks\n",
    "not_cm_peaks_df = utils.read_bed(\n",
    "    path_to_file_dir=os.path.join(path_to_cm_peaks, \"peak_files\"),\n",
    "    file_name=\"_\".join([dataset, \"not\", method, \"peaks.bed\"]),\n",
    "    sep=\"\\t\",\n",
    "    add_chr=True,\n",
    "    add_chr_col=\"chr\",\n",
    "    columns=[\"chr\", \"peak_start\", \"peak_end\", \"peak_id\", \"peak_id_dup\", \"strand\"],\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "# Exclude not CM peaks that overlap CM peaks\n",
    "peaks_to_exclude_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        overlapping_peaks_path,\n",
    "        \"_\".join([dataset, method, \"not\", method, \"peaks_overlap.bed\"]),\n",
    "    ),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    ")\n",
    "peaks_to_exclude_df.columns = [\n",
    "    \"chr_1\",\n",
    "    \"start_1\",\n",
    "    \"end_1\",\n",
    "    \"pid_1\",\n",
    "    \"cm_id_1\",\n",
    "    \"strand_1\",\n",
    "    \"chr_2\",\n",
    "    \"start_2\",\n",
    "    \"end_2\",\n",
    "    \"pid_2\",\n",
    "    \"pid_2_dup\",\n",
    "    \"strand_2\",\n",
    "    \"ovrlp_len\",\n",
    "]\n",
    "peaks_overlapping_cm_peaks = set(peaks_to_exclude_df.loc[:, \"pid_1\"]).union(\n",
    "    set(peaks_to_exclude_df.loc[:, \"pid_2\"])\n",
    ")\n",
    "# Exclude not CM H3K27ac peaks that overlap not CM peaks.\n",
    "# The reason to exclude H3K27ac peaks is that they are broader and\n",
    "# therefore can overlap more H3K4me1 peaks\n",
    "k27ac_peaks_ovrlp_others = np.load(\n",
    "    os.path.join(\n",
    "        path_to_cm_peaks,\n",
    "        \"peak_overlaps\",\n",
    "        \"overlapping_marks_peak_mapping\",\n",
    "        method,\n",
    "        \"_\".join(\n",
    "            [\n",
    "                dataset,\n",
    "                \"overlapping_not\",\n",
    "                method,\n",
    "                \"peaks_k27ac_to_k4me1_mapping.npy\",\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    allow_pickle=True,\n",
    ").item()\n",
    "\n",
    "all_possible_peaks_to_exclude = list(\n",
    "    peaks_overlapping_cm_peaks.union(set(cm_peaks_df.loc[:, \"peak_id\"])).union(\n",
    "        set(k27ac_peaks_ovrlp_others.keys())\n",
    "    )\n",
    ")\n",
    "\n",
    "not_cm_peaks_df = not_cm_peaks_df.loc[\n",
    "    ~not_cm_peaks_df.loc[:, \"peak_id\"].isin(all_possible_peaks_to_exclude), :\n",
    "].copy()\n",
    "single_peak_df = single_peak_df.loc[\n",
    "    ~single_peak_df.loc[:, \"peak_id\"].isin(all_possible_peaks_to_exclude), :\n",
    "]\n",
    "single_peak_df = single_peak_df.loc[\n",
    "    single_peak_df.loc[:, \"peak_id\"].isin(not_cm_peaks_df.loc[:, \"peak_id\"]), :\n",
    "]\n",
    "\n",
    "not_cm_peak_coordinates_by_chr_dict = utils.get_peak_coord_by_chr(\n",
    "    peak_df=not_cm_peaks_df,\n",
    "    chr_col=\"chr\",\n",
    "    peak_id_col=\"peak_id\",\n",
    "    peak_start_col=\"peak_start\",\n",
    "    peak_end_col=\"peak_end\",\n",
    ")\n",
    "count_mtx_k4me1 = utils.read_count_mtx(\n",
    "    count_mtx_path,\n",
    "    \"H3K4me1_chr22.bed\",\n",
    "    \"H3K4me1\",\n",
    "    chromosomes=chromosomes,\n",
    "    add_marks=True,\n",
    "    add_chr=True,\n",
    ")\n",
    "count_mtx_k27ac = utils.read_count_mtx(\n",
    "    count_mtx_path,\n",
    "    \"H3K27ac_chr22.bed\",\n",
    "    \"H3K27ac\",\n",
    "    chromosomes=chromosomes,\n",
    "    add_marks=True,\n",
    "    add_chr=True,\n",
    ")\n",
    "\n",
    "# Merge count matrices per mark into one common matrix\n",
    "count_mtx = pd.concat([count_mtx_k4me1, count_mtx_k27ac])\n",
    "\n",
    "count_mtx_dict = {\n",
    "    chromosome: count_mtx_chr.set_index(\"pid\").iloc[:, 5:].astype(float)\n",
    "    for chromosome, count_mtx_chr in count_mtx.groupby(\"chr\")\n",
    "}\n",
    "if save_files:\n",
    "    if not os.path.exists(os.path.join(output_data_path, dataset)):\n",
    "        os.makedirs(os.path.join(output_data_path, dataset))\n",
    "    np.save(\n",
    "        os.path.join(output_data_path, dataset, \"peak_count_matrix_dict_by_chr.npy\"),\n",
    "        count_mtx_dict,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    single_peak_df.to_csv(\n",
    "        os.path.join(output_data_path, dataset, \"single_peak_df_for_simulation.txt\"),\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )\n",
    "\n",
    "peak_dfs_dict = {\"cm_peaks\": cm_peaks_df, \"not_cm_peaks\": not_cm_peaks_df}\n",
    "\n",
    "tracks_df, _ = utils.get_cm_tracks_content(\n",
    "    os.path.join(core_path, \"1.mapped_CMs\"), method, dataset, pp_threshold=pp_threshold\n",
    ")\n",
    "tracks_df.columns = [\n",
    "    \"chr\",\n",
    "    \"start\",\n",
    "    \"end\",\n",
    "    \"cm_id\",\n",
    "    \"number\",\n",
    "    \"strain\",\n",
    "    \"start_duplicate\",\n",
    "    \"end_duplicate\",\n",
    "    \"numbers\",\n",
    "    \"cm_size\",\n",
    "    \"peak_length\",\n",
    "    \"peak_starts\",\n",
    "    \"is_totem\",\n",
    "]\n",
    "tracks_df.loc[:, \"cm_length\"] = tracks_df.loc[:, \"end\"] - tracks_df.loc[:, \"start\"]\n",
    "tracks_df.loc[:, \"cm_id\"] = tracks_df.loc[:, \"cm_id\"].str.replace(\"_\", \"~\")\n",
    "\n",
    "# Prepare dictionary with peak coordinates.\n",
    "# Consider two groups of peaks: CM- and not CM-peaks\n",
    "# Get peak coordinates by CM and chromosome\n",
    "cm_peak_coordinates_by_chr = utils.get_peak_coord_in_cms_by_chr(\n",
    "    cm_peaks_df=peak_dfs_dict[\"cm_peaks\"],\n",
    "    chr_col=\"chr\",\n",
    "    peak_start_col=\"peak_start\",\n",
    "    peak_end_col=\"peak_end\",\n",
    "    cm_id_col=\"cm_id\",\n",
    ")\n",
    "# Get peak coordinates by chromosome for not CM-peaks\n",
    "not_cm_peak_coordinates_by_chr = utils.get_peak_coord_by_chr(\n",
    "    peak_df=peak_dfs_dict[\"not_cm_peaks\"],\n",
    "    chr_col=\"chr\",\n",
    "    peak_id_col=\"peak_id\",\n",
    "    peak_start_col=\"peak_start\",\n",
    "    peak_end_col=\"peak_end\",\n",
    ")\n",
    "if save_files:\n",
    "    np.save(\n",
    "        os.path.join(output_data_path, dataset, \"cm_peak_coordinates_dict_by_chr.npy\"),\n",
    "        cm_peak_coordinates_by_chr,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "    np.save(\n",
    "        os.path.join(\n",
    "            output_data_path, dataset, \"not_cm_peak_coordinates_dict_by_chr.npy\"\n",
    "        ),\n",
    "        not_cm_peak_coordinates_by_chr,\n",
    "        allow_pickle=True,\n",
    "    )\n",
    "\n",
    "cm_param_df = tracks_df.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"chr\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "        \"cm_id\",\n",
    "        \"cm_size\",\n",
    "        \"cm_length\",\n",
    "        \"peak_length\",\n",
    "        \"peak_starts\",\n",
    "    ],\n",
    "].copy()\n",
    "\n",
    "cm_param_df.loc[:, \"mean_d\"] = np.nan\n",
    "cm_param_df.loc[:, \"H3K27ac_fraction\"] = np.nan\n",
    "for index, row in cm_param_df.iterrows():\n",
    "    starts = (\n",
    "        np.array([int(p_s) for p_s in row[\"peak_starts\"].split(\",\")]) + row[\"start\"]\n",
    "    )\n",
    "    ends = starts + np.array([int(p_len) for p_len in row[\"peak_length\"].split(\",\")])\n",
    "    start_end = list(zip(starts, ends))\n",
    "\n",
    "    ends = np.array([se_pair[1] for se_pair in start_end[:-1]])\n",
    "    pairwise_peak_dist = []\n",
    "    for i in range(1, len(start_end)):\n",
    "        lst = np.array([se_pair[0] for se_pair in start_end[i:]])\n",
    "        pairwise_peak_dist.extend(lst - np.ravel([ends[i - 1]] * len(lst)))\n",
    "    cm_param_df.loc[index, \"mean_d\"] = np.mean(pairwise_peak_dist)\n",
    "    cm_param_df.loc[index, \"H3K27ac_fraction\"] = mark_counts_dict.get(row[\"cm_id\"])\n",
    "\n",
    "if save_files:\n",
    "    cm_param_df.to_csv(\n",
    "        os.path.join(output_data_path, dataset, \"cm_parameter_df.bed\"),\n",
    "        sep=\"\\t\",\n",
    "        header=True,\n",
    "        index=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
